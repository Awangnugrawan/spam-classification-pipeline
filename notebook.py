# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_LI89LqHLaD63rCX96c8or1yYjjVbbZd

# Submission Proyek Pengembangan Machine Learning Pipeline

Nama = Awang Mulya Nugrawan \
username Dicoding = awangnugrawan

# IMPORT LIBRARY
"""

import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os
import pandas as pd
from tfx.proto import trainer_pb2
from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing
import tensorflow_model_analysis as tfma
from tfx.components import Pusher
from tfx.proto import pusher_pb2

"""## Set Variable"""

PIPELINE_NAME = "Awang Nugrawan-pipeline"
SCHEMA_PIPELINE_NAME = "spam-tfdv-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

DATA_ROOT = "data"

"""# DATASET INFORMATION"""

df= pd.read_csv('data\SPAM text message 20170820 - Data.csv')

df

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""# Data Ingestion

Proses Data Ingestion ini melibatkan pengambilan data dari sumber eksternal dan pemrosesan awal untuk persiapan data dalam konteks proyek machine learning. Dalam contoh ini, kita menggunakan CsvExampleGen untuk mengambil data dari direktori "data" sebagai sumber input. Data ini kemudian dibagi menjadi dua bagian, yaitu "train" dan "eval," dengan konfigurasi pembagian yang menghasilkan 8 bucket untuk "train" dan 2 bucket untuk "eval." Ini memungkinkan data dibagi secara acak dengan pembagian yang seimbang untuk melatih dan menguji model machine learning. Proses ini akan menghasilkan dataset yang siap untuk digunakan dalam proses selanjutnya dalam proyek machine learning.
"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2)
    ])
)
example_gen = CsvExampleGen(input_base="data", output_config=output)

interactive_context.run(example_gen)

"""# Data Validation

## summary statistic

Proses Data Validation ini melibatkan penggunaan StatisticsGen untuk menghasilkan statistik deskriptif dari dataset yang dihasilkan dari proses Data Ingestion sebelumnya. Statistik ini mencakup informasi seperti rata-rata, deviasi standar, dan rangkuman lainnya tentang fitur-fitur dalam data.
"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)

interactive_context.run(statistics_gen)

"""Setelah statistik tersebut dihasilkan, kita dapat menampilkannya menggunakan interactive_context.show untuk memahami karakteristik dataset dan mendeteksi anomali atau ketidaksesuaian dalam data yang mungkin perlu diperbaiki sebelum proses analisis lebih lanjut. Proses ini membantu memastikan kualitas data yang baik sebelum digunakan dalam pengembangan model machine learning."""

interactive_context.show(statistics_gen.outputs["statistics"])

"""## Data schema

Proses Data Validation ini melibatkan penggunaan SchemaGen untuk menghasilkan skema data berdasarkan statistik yang dihasilkan dari proses Data Ingestion dan Data Statistics sebelumnya. Skema ini mendefinisikan tipe data dan batasan-batasan untuk setiap fitur dalam dataset, sehingga memungkinkan untuk melakukan validasi lebih lanjut terhadap data yang masuk.
"""

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"]
)
interactive_context.run(schema_gen)

"""Setelah skema data dihasilkan, kita dapat menampilkannya menggunakan interactive_context.show untuk memeriksa secara visual struktur dan tipe data dalam dataset, yang dapat membantu dalam memastikan kualitas data dan pemahaman yang lebih baik tentang dataset yang digunakan dalam proyek machine learning"""

interactive_context.show(schema_gen.outputs["schema"])

"""## Identifying anomalies in the dataset

Proses Data Validation ini melibatkan penggunaan ExampleValidator untuk melakukan validasi lebih lanjut terhadap dataset dengan membandingkan statistik yang dihasilkan dari proses Data Ingestion dan skema data yang telah dihasilkan sebelumnya. Tujuan dari langkah ini adalah untuk mendeteksi anomali atau ketidaksesuaian dalam data, seperti nilai yang diluar batas tipe data yang didefinisikan dalam skema.
"""

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

"""Hasil dari validasi ini, yang disebut "anomalies," akan ditampilkan menggunakan interactive_context.show, memungkinkan pemahaman yang lebih baik tentang masalah atau kesalahan dalam dataset yang perlu diperbaiki sebelum digunakan dalam pengembangan model machine learning."""

interactive_context.show(example_validator.outputs['anomalies'])

"""Berdasarkan hasil tersebut pada dataset tidak ditemukan adanya anomaly

# Data Preprocessing

Proses Data Preprocessing ini melibatkan penggunaan Transform untuk melakukan persiapan data sebelum digunakan dalam pelatihan model machine learning. Dalam kasus ini, kita menggunakan sebuah fungsi preprocessing yang didefinisikan dalam modul "spam_transform.py" untuk mengubah fitur-fitur input menjadi fitur-fitur yang telah diubah sesuai dengan kebutuhan model.
"""

TRANSFORM_MODULE_FILE = "spam_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# import tensorflow as tf
# LABEL_KEY = "Category"
# FEATURE_KEY = "Message"
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# def preprocessing_fn(inputs):
#     """
#     Preprocess input features into transformed features
# 
#     Args:
#         inputs: map from feature keys to raw features.
# 
#     Return:
#         outputs: map from feature keys to transformed features.
#     """
# 
#     outputs = {}
# 
# 
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
# 
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
# 
#     return outputs

"""Hasil dari proses ini adalah dataset yang telah diubah, siap untuk digunakan dalam pelatihan model. Proses ini memastikan bahwa data telah dipersiapkan dengan benar dan dapat digunakan dalam model machine learning dengan baik."""

transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)
interactive_context.run(transform)

"""# Model Training

Proses Data Training ini melibatkan penggunaan modul "spam_trainer.py" untuk melatih model machine learning dengan menggunakan data yang telah diproses sebelumnya. Dalam proses ini, pertama-tama kita membangun arsitektur model dengan menggunakan TensorFlow, yang mencakup lapisan-lapisan seperti TextVectorization untuk mengubah teks menjadi representasi numerik, lapisan embedding, lapisan GlobalAveragePooling1D, dan lapisan-lapisan lain untuk pembelajaran. Model ini dikompilasi dengan loss function 'binary_crossentropy' untuk tugas klasifikasi biner dan optimizer Adam dengan learning rate 0.01.

Selanjutnya, kita mempersiapkan fungsi untuk melayani contoh-contoh TensorFlow dan memuat model dengan lapisan transformasi dari output TensorFlow Transform. Proses pelatihan model menggunakan data yang telah diproses sebelumnya dengan callback seperti TensorBoard, EarlyStopping, dan ModelCheckpoint untuk pemantauan pelatihan.
"""

TRAINER_MODULE_FILE = "spam_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = "Category"
# FEATURE_KEY = "Message"
# 
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# 
# def input_fn(file_pattern,
#              tf_transform_output,
#              num_epochs,
#              batch_size=64)->tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = transformed_name(LABEL_KEY))
#     return dataset
# 
# 
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE = 12000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH)
# 
# 
# embedding_dim=16
# def model_builder():
#     """Build machine learning model"""
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation="relu")(x)
#     x = layers.Dense(32, activation="relu")(x)
#     x = layers.Dense(16, activation="relu")(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
# 
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
# 
#     )
# 
#     # print(model)
#     model.summary()
#     return model
# 
# 
# def _get_serve_tf_examples_fn(model, tf_transform_output):
# 
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
# 
#         feature_spec = tf_transform_output.raw_feature_spec()
# 
#         feature_spec.pop(LABEL_KEY)
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
# 
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs) -> None:
# 
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = log_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=7)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
# 
# 
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(FEATURE_KEY)]
#                 for i in list(train_set)]])
# 
#     # Build the model
#     model = model_builder()
# 
# 
#     # Train the model
#     model.fit(x = train_set,
#             validation_data = val_set,
#             callbacks = [tensorboard_callback, es, mc],
#             steps_per_epoch = 1000,
#             validation_steps= 1000,
#             epochs=10)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }
#     model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

trainer  = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)
interactive_context.run(trainer)

"""# Model Evaluation

## Resolver

Proses Model Evaluation ini melibatkan penggunaan komponen LatestBlessedModelStrategy yang bertugas untuk memilih model yang telah disetujui (blessed model) terbaru. Dalam langkah ini, resolver (model_resolver) digunakan untuk menentukan model mana yang dianggap sebagai model yang telah dilewatkan (blessed model) terbaru berdasarkan berbagai kriteria tertentu, seperti kualitas atau performa.
"""

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

"""## Evaluator

Proses Model Evaluation ini melibatkan konfigurasi evaluasi dengan menggunakan EvalConfig dari TensorFlow Model Analysis (TFMA). Konfigurasi ini berisi spesifikasi model yang diuji, metrik yang akan dihitung, dan parameter-parameter evaluasi lainnya.

- model_specs menentukan model yang akan dievaluasi, dengan label kunci 'Category'.
- slicing_specs mendefinisikan opsi pemotongan data (slice), dalam contoh ini, tidak ada pemotongan data khusus yang ditentukan.
- metrics_specs mengkonfigurasi metrik yang akan dihitung, termasuk metrik seperti jumlah contoh, AUC, False Positives, True Positives, False Negatives, True Negatives, dan Binary Accuracy. Binary Accuracy memiliki ambang batas nilai dan perubahan tertentu yang harus dicapai untuk dianggap berhasil.
"""

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='Category')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

"""Selanjutnya, digunakan komponen Evaluator untuk melakukan evaluasi terhadap model yang telah dilatih dan model baseline (model terbaru yang telah dilewatkan). Hasil evaluasi ditampilkan dan divisualisasikan dengan menggunakan TensorFlow Model Analysis (TFMA), yang memungkinkan untuk memahami kualitas model, metrik, serta indikator keadilan dalam pemrosesan data. Dengan demikian, proses ini membantu dalam penilaian dan pemahaman kinerja model machine learning yang telah dikembangkan."""

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""# Pusher

Proses pusher (pengunggahan) ini melibatkan komponen Pusher yang bertanggung jawab untuk mengunggah (mempublikasikan) model yang telah dilatih ke suatu tempat yang dapat diakses untuk penggunaan praktis. Dalam konteks ini, model yang akan diunggah berasal dari keluaran Trainer, dan pengunggahan model ini bergantung pada hasil evaluasi dari komponen Evaluator.

- Model yang diunggah adalah model yang telah dilatih (trainer.outputs['model']).
- Model blessing, yang merupakan hasil dari evaluasi model (evaluator.outputs['blessing']), digunakan untuk memastikan bahwa model yang akan diunggah telah memenuhi persyaratan kualitas dan performa yang ditetapkan.
- Push Destination (tujuan pengunggahan) diatur sebagai direktori filesystem ('serving_model_dir/spam-detection-model') di mana model akan disimpan.
"""

pusher = Pusher(
model=trainer.outputs['model'],
model_blessing=evaluator.outputs['blessing'],
push_destination=pusher_pb2.PushDestination(
    filesystem=pusher_pb2.PushDestination.Filesystem(
        base_directory='serving_model_dir/spam-detection-model'))

)

interactive_context.run(pusher)